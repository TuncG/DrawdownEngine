import numpy as np
import pandas as pd
import yfinance as yf               # quick data source
from arch import arch_model
import sys
# ------------------------------
# 1.  Load data
# ------------------------------
def get_data(ticker, start):
    """Load adjusted close prices from Yahoo Finance."""
    prices = yf.download(ticker, start=start, progress=False)
    prices = prices["Close"]
    prices = prices.dropna()                       # drop missing values
    return prices

def get_log_returns(prices):
    """Calculate log-returns from prices."""
    logret = np.log(prices).diff().dropna() * 100   # % log‑returns (scale helps with numbers)
    return logret



def bootstrap_mdar_ced(logret,
                       prices,
                       B=1000,               # number of bootstrap replicates
                       horizon=252,
                       block=20,             # circular block length 
                       alpha=0.95,
                       seed=42):
    """
    Parametric residual bootstrap for draw‑down risk measures.

    1. Fit AR(1)-GJR-GARCH(1,1)-t ---> get cond. means μ_t, cond. std σ_t,
       and standardised residuals z_t = (r_t-μ_t)/σ_t.
    2. Block‑bootstrap the z_t's (keeps short‑range dependence).
    3. Rebuild return paths:  r*_t = μ_t + σ_t * z*_t
    4. Evaluate MDaR / CED on each path.
    """
    rng = np.random.default_rng(seed)

    # ---------- 1. Fit model once  ----------
    am  = arch_model(logret, mean="ARX", lags=1,
                     vol="GARCH", p=1, o=1, q=1, dist="t")
    fit = am.fit(update_freq=0, disp="off")

    cond_mean = fit.params['mu'] + fit.params['ar.L1'] * logret.shift(1)
    cond_sigma = fit.conditional_volatility
    z = (logret - cond_mean) / cond_sigma   # std.resids

    # pre‑pack numpy arrays for speed
    z = z.dropna().values
    mu = cond_mean.dropna().values
    sig= cond_sigma.dropna().values

    T = len(z)
    n_blocks = int(np.ceil(horizon / block))

    # ---------- 2–4. Bootstrap loop ----------
    mdar_b = np.empty(B)
    ced_b  = np.empty(B)

    for b in range(B):
        # 2. circular block bootstrap of z_t
        start_idx = rng.integers(0, T, size=n_blocks)
        idx = (start_idx[:, None] + np.arange(block)) % T
        z_star = z[idx].reshape(-1)[:horizon]

        # 3. rebuild returns and prices
        mu_star  = mu[-horizon:]       # use last 'horizon' conditional means
        sig_star = sig[-horizon:]
        r_star   = mu_star + sig_star * z_star         # still in % units
        p0 = prices.iloc[-horizon-1]                   # last observed price
        path = p0 * np.exp((r_star/100).cumsum())

        # 4. risk measures
        dd = np.maximum.accumulate(path) - path
        maxdd = dd.max()

        mdar_b[b] = maxdd
        ced_b[b]  = np.nan  # placeholder, fill after loop

    # CED uses all maxima that exceed the α‑quantile *of that sample*
    q = np.quantile(mdar_b, alpha)
    ced_b = mdar_b[mdar_b > q].mean()

    return mdar_b, ced_b



def simulate_garch(prices, logret,
                   horizon=252,
                   n_paths=5000,
                   burn=500,
                   seed=123):
    """
    Return an array of shape (n_paths , horizon) with simulated price levels
    generated by an AR(1)‑GJR‑GARCH(1,1) with t innovations.
    """

    # ---------- 1.  Fit the model ----------
    am = arch_model(logret, mean="ARX", lags=1,
                    vol="GARCH", p=1, o=1, q=1,
                    dist="t")
    res = am.fit(update_freq=0, disp="off")

    np.random.seed(seed)
    ret_paths = np.empty((n_paths, horizon))

    for i in range(n_paths):
        sim = am.simulate(res.params, nobs=horizon, burn=burn)
        ret_paths[i] = sim["data"].values        # still %‑units, keep them

    p0 = float(prices.iloc[-1])
    # convert % back to raw log units ONLY here
    price_paths = p0 * np.exp((ret_paths / 100).cumsum(axis=1))
    return price_paths

def max_drawdown(path):
        cummax = np.maximum.accumulate(path)
        drawdown = cummax - path
        return drawdown.max()

def main():
    # Load data
    ticker = "AAPL"
    start_date = "2016-01-01"
    prices = get_data(ticker, start_date)

    # Calculate log-returns
    logret = get_log_returns(prices)

    price_paths = simulate_garch(prices, logret)

    np.savetxt("simulated_prices.csv", price_paths, delimiter=",", fmt="%.6f")


    mdds = np.fromiter((max_drawdown(p) for p in price_paths), float)

    np.savetxt("simulated_mdds.csv", mdds, delimiter=",", fmt="%.6f")
 

    alpha  = 0.95
    mdar   = np.quantile(mdds, alpha)                 # MDaR (drawdown‑VaR)
    ced    = mdds[mdds > mdar].mean()                # CED (drawdown‑ES)
    print(f"MDaR 95%  : {mdar:.2f}")
    print(f"CED  beyond: {ced:.2f}")

main()